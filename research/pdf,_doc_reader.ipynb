{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aej7zZbLcahA",
        "outputId": "12af0eb1-f57f-4035-fa91-67cc8828ae8b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.0-py3-none-any.whl (56 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.29.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.0 pypdfium2-4.29.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Replace 'your_pdf_file.pdf' with the path to your PDF file\n",
        "pdf_text = extract_text_from_pdf('/content/RENGA RAJAN K.pdf')\n",
        "print(pdf_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoFrWge5bY6_",
        "outputId": "32bb15fc-2a05-4a9e-96e9-141975e1e55f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RENGA RAJAN K\n",
            "AI/ML Engineer\n",
            "maharengarajan46@gmail.com +91 9043450829 GitHub LinkedIn\n",
            "About Me\n",
            "I have 4+ years of experience in Machine Learning with a proven ability and history of developing\n",
            "full-stack machine learning projects. I'm curious about data, training Machine Learning/ Deep\n",
            "Learning models, and providing beautiful insights that are easily understandable. Hands-on\n",
            "experience in leveraging machine learning, deep learning, computer vision models to solve\n",
            "challenging business problems.\n",
            "Professional Experience\n",
            "Datanetiix Solutions Inc | AI/ML Engineer\n",
            "04/2023 – Present\n",
            "Key responsibilities:\n",
            "Design, develop, and implement end-to-end machine learning projects leveraging MLOps techniques,\n",
            "involving the development of robust pipelines for data processing, model training, and deployment.\n",
            "Develop and maintain automated workflows for model training, testing and deployment, optimizing for\n",
            "efficiency and repeatability.\n",
            "Collaborate with DevOps teams and other CFT teams to align machine learning processes with broader\n",
            "software development and deployment practices\n",
            "Foxconn Hon Hai Tech | ML Engineer\n",
            "09/2021 – 04/2023\n",
            "Key responsibilities:\n",
            "Involved in Data Pre-processing Techniques such as data cleaning, EDA, visualizing the data, identifying\n",
            "outliers and making data ready for Machine Learning Techniques\n",
            "Experience in creating mature Data science pipelines encompassing Data standardization, Feature\n",
            "extraction, model validation and optimization\n",
            "Apply multiple ML&AI algorithm to get the better result for the problem\n",
            "SV Global Logistics | Logistics Analyst\n",
            "05/2019 – 08/2021\n",
            "Key responsibilities:\n",
            "In this role, I'm responsible for creating and reviewing Standard Operating Procedures to streamline\n",
            "logistics processes. I stay informed about industry trends and utilize this knowledge in logistics planning.\n",
            "A key aspect of my role is closely monitoring the product flow, ensuring a smooth journey from origin to\n",
            "final delivery\n",
            "Pfizer Healthcare India| Junior Officer\n",
            "05/2015 – 02/2019\n",
            "Key responsibilities:\n",
            "I create and follow monthly maintenance plans, track equipment breakdowns, figure out why they\n",
            "happen, and fix them. I work with vendors to upgrade systems, and I manage supplies and equipment\n",
            "efficiently.Projects\n",
            "Pathogen Monitoring System\n",
            "I successfully developed a disease prediction model using machine learning techniques as part of an\n",
            "independent project. Leveraging a dataset of medical symptoms, I employed a classification\n",
            "approach to predict various diseases such as fungal infections and hypertension. The project\n",
            "showcased my proficiency in data pre-processing, feature engineering, and model training,\n",
            "highlighting my ability to apply machine learning to real-world problems and contribute to healthcare\n",
            "solutions.\n",
            "Chatbot\n",
            "I played a pivotal role in crafting a versatile system that combines rule-based interactions with\n",
            "advanced AI capabilities. This innovative approach ensures a seamless user experience by\n",
            "incorporating predefined rules for specific queries while harnessing the power of AI to handle more\n",
            "complex and nuanced conversations. By integrating natural language processing and machine\n",
            "learning, the chatbot adapts and learns from user interactions over time, continually improving its\n",
            "ability to understand and respond effectively.\n",
            "Object Detection - Heart Beam Mobile Application\n",
            "Led the development of an Object Detection system using TensorFlow, training a robust model to\n",
            "identify and localize objects in images. Leveraged TensorFlow's capabilities to ensure accurate and\n",
            "efficient detection. The trained model was then converted to TensorFlow Lite (tflite) format for\n",
            "seamless integration into mobile applications.\n",
            "Web Scraper\n",
            "Developed a robust Python-based web scraping application to extract data from LinkedIn profiles at\n",
            "regular intervals of every 4 hours. Implemented efficient scraping techniques to gather\n",
            "comprehensive details, enhancing data accuracy. The extracted information, including user profiles\n",
            "and relevant details, is stored efficiently in a MySQL database for easy retrieval and analysis.\n",
            "Education Background\n",
            "Bachelor of Computer Applications\n",
            "Pursuing the 3rd year at TNOU\n",
            "2022 - 2024\n",
            "Diploma in Mechanical Engineering\n",
            "P.A.C Ramasamy Raja Polytechnic College\n",
            "2012 - 2015\n",
            "SSLC\n",
            "st. John’s Hr Sec School\n",
            "2010 - 2012\n",
            "Skills\n",
            "Python Pandas Scikit-Learn Git\n",
            "Machine Learning Numpy Keras Github Actions\n",
            "Deep Learning Matplotlib TensorFlow DVC\n",
            "Computer Vision MySQL Open CV MLflow\n",
            "NLP Flask Jupyter notebook Docker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oUXEKnAcQqR",
        "outputId": "1369356f-0b36-4062-bd05-2bb84edf2472"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/239.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m235.5/239.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.10.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx\n",
        "\n",
        "def extract_text_from_docx(docx_path):\n",
        "    text = \"\"\n",
        "    doc = docx.Document(docx_path)\n",
        "    for paragraph in doc.paragraphs:\n",
        "        text += paragraph.text + '\\n'\n",
        "    return text\n",
        "\n",
        "# Replace 'your_word_file.docx' with the path to your Word document\n",
        "docx_text = extract_text_from_docx('/content/Resume Data Extraction.docx')\n",
        "print(docx_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0R7Q-nJiyaT",
        "outputId": "30ae052d-4e5d-4ef5-ea2c-0c96e90818e4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlining Resume Data Extraction:\n",
            "Executive Summary:\n",
            "\n",
            "Our proposal presents a solution for extracting employment history from resumes selected from the local disk. We propose a method to scrape the company worked data of individuals and download it in CSV format. This solution aims to simplify the process of extracting and managing employment history from resumes, improving efficiency and data accessibility. \n",
            "\n",
            "Key Points:\n",
            "\n",
            "File Selection: Utilize a file picker interface to select resumes from the local disk.\n",
            "Data Extraction: Use a scraping tool to extract company work data from the selected resumes.\n",
            "CSV Export: Format the extracted data into a CSV file for easy download and analysis.\n",
            "User Interface: Design a user-friendly interface to manage the selection and extraction process seamlessly.\n",
            "\n",
            "Proposal for Using NER and Resume Parsing Packages:\n",
            "\n",
            "NER (Named Entity Recognition):\n",
            "Utilize NER to identify entities such as company names, job titles, and dates within the resume text.\n",
            "Use pre-trained NER models such as SpaCy or NLTK to extract relevant information.\n",
            "\n",
            "Resume Parser Packages:\n",
            "Explore existing resume parsing packages like Pyresparser, ResumeParser, or Docparser.\n",
            "These packages can automatically extract structured data from resumes, including company names and job history.\n",
            "\n",
            "Data Extraction Pipeline:\n",
            "Develop a data extraction pipeline that combines NER and resume parser packages.\n",
            "Use NER to identify relevant entities and pass them to the resume parser for detailed extraction.\n",
            "\n",
            "CSV Output:\n",
            "Format the extracted data into a CSV file for easy download and analysis.\n",
            "Include fields such as company name, job title, start and end dates, and any other relevant information.\n",
            "\n",
            "User Interface Enhancement:\n",
            "Improve the user interface to allow seamless selection of resumes from the local disk.\n",
            "Provide feedback on the extraction process and allows users to review and edit extracted data before downloading.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-magic\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niQRrgB3khue",
        "outputId": "e4adeb52-f10f-41a4-d282-c422713b829e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-magic\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Installing collected packages: python-magic\n",
            "Successfully installed python-magic-0.4.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import magic\n",
        "\n",
        "def detect_document_format(file_path):\n",
        "    mime = magic.Magic(mime=True)\n",
        "    file_type = mime.from_file(file_path)\n",
        "    if 'pdf' in file_type:\n",
        "        return 'pdf'\n",
        "    elif 'word' in file_type:\n",
        "        return 'docx'\n",
        "    elif 'image' in file_type:\n",
        "        return 'image'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "# Example usage:\n",
        "file_path = '/content/Resume Data Extraction.docx'\n",
        "file_format = detect_document_format(file_path)\n",
        "print(\"Document format:\", file_format)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHptFq53jCgG",
        "outputId": "0d2ea4dc-c5aa-4492-ff79-9b5b45412919"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document format: docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tqnUmlxdkpo5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}